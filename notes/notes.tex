\documentclass[12pt,a4paper]{article}



\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{marginnote}
\usepackage{natbib}





\author{Jonathan Rosenblatt}
\title{Class Notes (experimental)}




\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}




\newcommand{\expect}[1]{E[#1]}
\newcommand{\expectn}[1]{\mathbb{E}[#1]}
\newcommand{\gauss}[1]{\mathcal{N}(#1)}
\newcommand{\cdf}[2]{F_{#1}(#2)}
\newcommand{\cdfn}[2]{\mathbb{F}_{#1}(#2)}
\newcommand{\icdf}[2]{F^{-1}_{#1}(#2)}
\newcommand{\icdfn}[2]{\mathbb{F}^{-1}_{#1}(#2)}
\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\lik}[1]{\mathcal{L}(#1)}
\newcommand{\loglik}[1]{L(#1)}
\newcommand{\loss}[1]{l(#1)}
\newcommand{\risk}[1]{R(#1)}
\newcommand{\riskn}[1]{\mathbb{R}(#1)}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\argmin}[2]{argmin_{#1}\left\{ #2 \right\}}
\newcommand{\hyp}{f}
\newcommand{\hypclass}{\mathcal{F}}
\newcommand{\plane}{\mathbb{L}}


\begin{document}

\maketitle


\section{Estimation}
\label{sec:estimation} 
In this section, we present several estimation principeles. 
Their properties are not discussed, as the section is merely a reminder and a preparation for Section~\ref{sec:learning}.
These concepts and examples can be found in many introductory books to statistics. I particularly recommend \citep{wasserman_all_2004}.

\subsection{Moment matching}
\label{sec:moment_matching}

The fundamental idea: match empirical moments to theoretical. I.e., estimate
$$ \expect{g(X)}   $$
by 
$$ \expectn{g(X)}   $$
where $\expectn{g(X)}:=\frac{1}{n}  \sum_i g(X_i)$, is the empirical mean.

\begin{example}[Exponential Rate]

Estimate $\lambda$ in $X_i \sim exp(\lambda)$, $i=1,\dots,n$, i.i.d.
$\expect{X}=1/\lambda$.
$\Rightarrow \hat{\lambda}=1/\expectn{X}$ 

\end{example}


\begin{example}[Linear Regression]

Estimate $\beta$ in $Y \sim \gauss{X\beta,\sigma^2 I}$, a $p$ dimensional random vector.
$\expect{Y}=X\beta$ and $\expectn{Y}=y$.
Clearly, moment mathing won't work because no $\beta$ satistifes $X\beta=Y$.
A technical workaround:
Since $\beta$ is $p$ dimensional, I need to find some $g(Y): \mathbb{R}^n \mapsto \mathbb{R}^p$.
Well, $g(Y):=XY$ is such a mapping. I will use it, even though my technical justification is currently unsatisfactory. We thus have:
$\expect{X'Y}=X'X\beta$ which I match to $\expectn{X'Y}=X'y$:
$$
  X'X \beta = X' y \Rightarrow \hat{\beta}=(X'X)^{-1} X'y.
$$

\end{example}


\subsection{Quantile matching}
\label{sec:quantiles}
The fundamental idea: match empirical quantiles to theoretical. 
Denoting by $\cdf{X}{t}$ the CDF of $X$, then $\icdf{X}{\alpha}$ is the $\alpha$ quantile of $X$.
Also denoting by $\cdfn{X}{t}$ the Empirical CDF of $X_1,\dots, X_n$, then $\icdfn{X}{\alpha}$ is the $\alpha$ quantile of $X_1,\dots, X_n$.
The quantile matching method thus implies estimating
$$ \icdf{X}{\alpha}   $$
by 
$$ \icdfn{X}{\alpha}  . $$

\begin{example}[Exponential rate]
Estimate $\lambda$ in $X_i \sim exp(\lambda)$, $i=1,\dots,n$, i.i.d.
\begin{align*}
	  \cdf{X}{t} =& 1-\exp(-\lambda t) = \alpha \Rightarrow \\
  \icdf{X}{\alpha} =& \frac{-\log(1-\alpha)}{\lambda} \Rightarrow \\
  \icdf{X}{0.5} =& \frac{-\log(0.5)}{\lambda} \Rightarrow \\
  \hat{\lambda} =& \frac{-\log(0.5)}{\icdfn{X}{0.5}}.
\end{align*}

\end{example}


\subsection{Maximum Likelihood}
\label{sec:ml}

The fundamental idea is that if the data generating proces (i.e., the \emph{sampling distribution}) can be assumed, then the observations are probably some high probability instance of this process, and not a low probability event:
Let $X_1,\dots,X_n \sim P_\theta$, with density (or probability) $p_\theta(X_1,\dots,X_n)$.
Denote the likelihood, as a function of $\theta$: $\lik{\theta}: p_\theta(X_1,\dots,X_n)$.
Then $\hat{\theta}_{ML}:= argmax_{\theta}\{ \lik{\theta} \}$.

\begin{example}[Exponential rate]

Estimate $\lambda$ in $X_i \sim exp(\lambda)$, $i=1,\dots,n$, i.i.d.
Using the exponential PDF and the i.i.d. assumption
$$ \lik{\lambda} = \lambda^n \exp(-\lambda \sum_i X_i). $$
Using a monotone mapping such as the log, does not change the $argmax$. 
Denoting $\loglik{\theta}:=\log(\lik{\theta})$, we have
$$ \loglik{\lambda} = n \log(\lambda) -\lambda \sum_i X_i. $$
By differentiating and equating $0$, we get $\hat{\lambda}_{ML}=1/\expectn{X}$.

\end{example}

\begin{example}[Discrete time Markov Chain]

Estimate the transition probabilities,  $p_1$ and $p_2$ in a two state, $\{0,1\}$, discrete time, Markov chain where:
$P(X_{t+1}=1|X_{t}=0)=p_1$ and $P(X_{t+1}=1|X_{t}=1)=p_2$.
The likelihood:
$$
  \lik{p_1,p_2}=P(X_1,\dots,X_n;p_1,p_2)=\prod_{t=0}^T P(X_{t+1}=x_{t+1}|X_{t}=x_t).
$$
We denote $n_{ij}$ the number of observed transitions from $i$ to $j$ and get that $\hat{p}_1=\frac{n_{01}}{n_{01}+n_{00}}$, and that $\hat{p}_2=\frac{n_{11}}{n_{11}+n_{10}}$.

\begin{remark}
Well, this is a rather artificial example, as because of the Markov property, and the stationarity of the process, we only need to look at transition events, themselves Brenoulli distributed. 
This example does show, however, the power of the ML method to deal with non i.i.d. samples. As does the next example.
\end{remark}
\end{example}

\begin{example}[Brownian motion with drift]
Estimate the drift parameter $a$,  in a discrete time Gaussian process where:
$X_{t+1}=X_t+\varepsilon; \varepsilon \sim \gauss{0,\sigma^2} \Rightarrow X_{t+1}|X_t \sim \gauss{a X_t,\sigma^2}$.

We start with the conditional density at time $t+1$:
$$
  p_{X_{t+1}|X_t=x_t}(x_{t+1}) = 
  (2 \pi \sigma^2)^{-1/2} \exp \left( 
    -\frac{1}{2 \sigma^2}(x_{t+1}-a x_t)^2 
  \right).
$$
Moving to the likelihood:
$$
  \lik{a} = 
  (2 \pi \sigma^2)^{-T/2} \exp \left(
    -\frac{1}{2 \sigma^2}\sum_{t=1}^T (x_{t+1}-a x_t)^2 
  \right).
$$
Differentiating with respect to $a$ and equating $0$ we get $\hat{a}_{ML}=\frac{\sum x_{t+1}x_{t}}{\sum x_t^2}$.

We again see the power of the ML device.
Could we have arrive to this estimator by intuiton alone? Hmmmm... maybe. 
See that $Cov[X_{t+1},X_t] = a \; Var[X_t] \Rightarrow a=\frac{Cov[X_{t+1},X_t]}{Var[X_t]}$.
So $a$ can also be derived using the moment matching method which is probably more intuitive.

\end{example}

\begin{example}[Linear Regression]

Estimate $\beta$ in $Y \sim \gauss{X\beta,\sigma^2 I}$, a $p$ dimensional random vector.
Recalling the multivariate Gaussian PDF:
$$
  p_{\mu,\Sigma}(y) = 
  (2 \pi)^{-n/2} |\Sigma|^{-1/2} \exp\left(
    -\frac{1}{2} (y-\mu)' \Sigma^{-1} (y-\mu)
  \right)
$$
So in the regression setup:
$$
  \lik{\beta}= 
  p_{\beta,\sigma^2}(y) = 
  (2 \pi)^{-n/2} |\sigma^2 I|^{-1/2} \exp\left(
    -\frac{1}{2 \sigma^2} \norm{y-X\beta}^2
  \right)
$$

\end{example}


\subsection{M-Estimation and Empirical Risk Minimization}
\label{sec:m_estimation}

M-Estimation, know as Empirical Risk Minimizaton (ERM) in the machine learning literature, is a very wide framework which stems from statistical desicion theory.
The underlying idea is that each realization of $X$ incurs some loss, and we seek to find a "policy", in this case a parameter, $\theta^*$ that minimizes the average loss.
In the econometric literature, we dot not incur a loss, but rather a utility, we thus seek a policy that maximizes the average utility.

Define a loss function $\loss{X;\theta}$, and a risk function, being the expected loss, 
$\risk{\theta}:=\expect{\loss{X;\theta}}$. Then 
\begin{align}
\label{eq:risk_min}
 \theta^*:= \argmin{\theta}{\risk{\theta}}.
\end{align}



As we do not know the distribution of $X$, we cannot solve Eq.(\ref{eq:risk_min}), so we minimize the \emph{empirical} risk. 
Define the empirical risk as $\riskn{\theta}:=\expectn{\loss{X;\theta}}$, then 
\begin{align}
\label{eq:empirical_risk_min}
 \hat{\theta}:= \argmin{\theta}{\riskn{\theta}}.
\end{align}





\begin{example}[Squared Loss]
Let $\loss{X;\theta}=(X-\theta)^2$. Then 
$
	\risk{\theta} = 
	\expect{(X-\theta)^2} = 
	(\expect{X}-\theta)^2 + Var[X]. 
$
Clearly $Var[X]$ does not depend on $\theta$  so that $\risk{\theta}$ is minimized by $\theta^*=\expect{X}$.
\textbf{We thus say that the expectation of a random variable is the minimizer of the squared loss.}

How do we estimate the population expectation? Well a natural estimator is the empirical mean, which is also the minimizer of the empirical risk $\riskn{X}$. The proof is immediate by differentiating. 
\end{example}



\begin{example}[Least Squares Regression]
\label{eg:OLS}
Define the loss $\loss{Y,X;\beta}:=\frac{1}{2}(Y-X\beta)^2$.
Computing the risk, $\expect{\norm{Y-X\beta}^2}$ will require dealing with the $X$'s by either assuming the \textbf{Generative Model}\footnote{A Generative Model is a supervised learning problem where the we use the assumed distribution of the $X$s and not only $Y|X$. The latter are know as Discriminative Models.}, as expectation is taken over $X$ and $Y$. 
We don't really care about that right now. 
We merely want to see that the empirical risk minimizer, is actually the classical OLS Regression. And well, it is, by definition...
\begin{align*}
	\riskn{\beta}=\sum_{i=1}^n 	\frac{1}{2}(y-x_i\beta)^2 = \frac{1}{2}\norm{y-x\beta}^2.
\end{align*}
Minimization is easiest with vector derivatives, but I will stick to regular derivatives:
\begin{align*}
	\deriv{\riskn{\beta}}{{\beta_j}} = \sum_i \left[ (y_i-\sum_{j=1}^p x_{ij}\beta_j)(-x_{ij}) \right]
\end{align*}
Equating $0$ yields $\hat{\beta_j}=\frac{\sum_i y_i x_{ij}}{\sum_i x_{ij}^2}$.
Solving for all $j$'s and putting in matrix notation we get
\begin{align}
	\hat{\beta}_{OLS}=(X'X)^{-1} X'y.
\end{align}


\end{example}


\subsection{Notes}
\paragraph{Maximum Likelhood} 
If we set the loss function to be the negative log likelihood of the (true) sampling distribution, we see that maximum likelihood estimators in independent samples are actually a certain type of M-estimators.


\section{From Estimation to Supervised Learning}
\label{sec:learning}
This section draws from \cite{hastie_elements_2003} and \cite{shalev-shwartz_understanding_2014}.
The former is freely available online.
For a softer introduction, with more hands-on examples, see \cite{james_introduction_2013}.
All books are very well written and strongly recommended.


\subsection{Empirical Risk Minimization (ERM) and Inductive Bias}
In Supervised Learning problems where we want to extract the relation $y=\hyp(x)$ between attributes $x$ and some outcome $y$.
In particular, we don't need to explain the causal process relating the two, so there is no need to commit to a sampling distribution. The implied M-Estimation problem is thus
\begin{align}
	\hat{\hyp}(x) = \argmin{\hyp}{\sum_i \loss{y_i - \hyp(x_i) }}.
\end{align}
Alas, there are clearly infinitely many $\hyp$ for which $\riskn{\hat{\hyp}(x)}=0$, in particular, all those where $\hat{\hyp}(x_i)=y_i$.
All these $\hyp$ feel like very bad predictors, and we will indeed formalize this intuition in Section~\ref{sec:desicion_theory}.

We need to make sure that we do not learn overly complex poor predictors. 
Motivated by the fact that humans approach new problems equipped with their past experience, this regularization is called \emph{Inductive Bias}.
There are several ways to introduce this bias, which can be combined:
\begin{description}
\item[The Hypothesis Class]
We typically do not allow $\hyp$ to be ``any function'' but rather restrict it to belong to a certain class. In the machine learning terminology, $\hyp$ is a Hypothesis, and it belongs to $\hypclass$ which is the Hypothesis Class.
\item[Prior Knowledge] We do not need to treat all $\hyp \in \hypclass$ equivalently. We might have prior preferences towards particular $\hyp$'s and we can introduce these preference in the learning process. This is called \emph{Regularization}.
\item[Non ERM Approaches] Many learning problems can be cast as ERM problems, but another way to introduce bias is by learning $\hyp$ via some other scheme, which cannot be cast as an ERM problem. 
Learning algorithms that cannot be cast as ERMs include: Nearest Neighbour, Kernel Smoothing, Boosting.
Naive Bayes and Fisher's LDA are also not considered ERMs, but they can be cast as such [TODO: verify].
\end{description}



We now proceed to show that many supervised learning algorithms are in fact ERMs with some type of inductive bias.

\subsection{Linear Regression (OLS)}
\label{sec:ols}
As seen in Example~\ref{eg:OLS}, by adopting a squared error loss, and restricting $\hypclass$ by assuming $f$ is a linear function of $x$, we get the OLS problem.
In this case, learning $f$ is effectively the same as learning $\beta$ as they are isomorphic.

\begin{remark}
We distinguish between OLS and Linear Regression. In these notes, we refer to linear regression when we assume that the data generating process it actually $y=x\beta+\varepsilon$, whereas in OLS we merely fit a linear function without claiming it is the data generating one.
\end{remark}



\subsection{Ridge Regression}
Consider the Ridge regression problem:
\begin{align}
\label{eq:ridge}
	& \argmin{\beta}{\frac{1}{n}\sum_i (y_i-x_i\beta)^2 + \frac{\lambda}{2}\norm{\beta}^2} \\
	& \hat{\beta}= (X'X+\lambda I)^{-1} X'y
\end{align}
We can see that again, $\hypclass$ is restricted to be the space of linear functions of $x$, but we also add a regularization that favors the linear functions with small coefficients.

The regularization of $\beta$ can have several interpretations and justifications.
\begin{description}
\item[A mathematical device] Strengthening the diagonal of $X'X$ makes is more easily invertible. This is a standard tool in applied mathematics called Tikhonov Regularization. It is also helpful when dealing with multicolinearity, as $(X'X+\lambda I)$ is always invertible.
\item[A Subjective Bayesian View] If we believe that $\beta$ should be small; say our believes can be quantified by $\beta \sim \gauss{0,\lambda I}$, then the Ridge solution is actually the mean of our posterior believes on $\beta|y$.
\end{description}

For more on Ridge regression see \cite{hastie_elements_2003}.


\subsection{LASSO}
Consider the LASSO problem:
\begin{align}
\label{eq:lasso}
	& \argmin{\beta}{\frac{1}{n}\sum_i (y_i-x_i\beta)^2 + \lambda \norm{\beta}^2_1} 
\end{align}
As can be seen, just like in Ridge regression, $\hypclass$ is restricted to linear functions. The regularization however differs. Instead of $l_2$ penalty, we use an $l_1$ penalty.
Eq.(\ref{eq:lasso}) does not have a closed form solution for $\hat{\beta}$ but the LARS algorithms solves it efficiently.

The LASSO has gained much popularity as it has the property that $\hat{\beta}$ has many zero entries. It is thus said to be \emph{sparse}.
The sparsity property is very attractive as it acts as a model selection method, allowing to consider $X$s where $p>n$, and making predictions computationally efficient.




\subsection{Logistic Regression}
The logistic regression is the first categorical prediction problem. I.e., the outcome $y$ is not a continous variable, but rather takes values in some finite set. In the logistic regression problem, it can take two possible values.
In the statistical literature, $y$ is encoded as $\{0,1\}$ and $\hyp$ is assumed to take to take the following form:
\begin{align}
	& P(y=1|x) = \Psi(x\beta) \\
	& \Psi(t) = \frac{1}{1+e^{-t}}
\end{align}

The hypothesis class $\hypclass$ is thus all $\hyp(x)=\Psi(x\beta)$.
In the $\{0,1\}$ encoding, the loss is the negative log likelihood, i.e.:
\begin{align}
	\loss{y,x,\beta} = -\log \left[ \Psi(x\beta)^{y} (1-\Psi(x\beta))^{1-y}  \right].
\end{align}
In the learning literature it is more common for $\{1,-1\}$ encoding of $y$ in which case the loss is 
\begin{align}
	\loss{y,x,\beta} = -\log \left[ 1+\exp(-y f(x))  \right].
\end{align}


\paragraph{How to classify?}
In the $\{0,1\}$ encoding, we predict class $1$ if $\Psi(x\beta)>0.5$ and class $0$ otherwise.
The logistic problem thus defines a separating hyperplane $\plane$ between the classes: $\plane=\{x:f(x)=0.5\}$.

In the $\{1,-1\}$ encoding, we predict class $1$ if $\Psi(x\beta)>0$ and class $0$ otherwise.
The plane $\plane$ is clearly invariant to the encoding of $y$.



\subsection{Regression Classifier}
\label{sec:regression_classifier}
[TODO]
Can we use the OLS framework for prediction with proper encoding of $y$?
It turns out we can.
Solving the same problem from Example~\ref{eg:OLS} with 

\subsection{Linear SVM}

\subsection{Generalized Additive Models (GAMs)}

\subsection{Neural Nets (NNETs)}

\subsection{Classification and Regression Trees (CARTs)}





\section{Unsupervised Learning}
\label{sec:unsupervised}
[TODO]

\section{Statistical Decision Theory}
\label{sec:desicion_theory}
[TODO]


\section{Dimensionality Reduction}
\label{sec:dim_reduce}
[TODO]


\section{Latent Space Models}
\label{sec:latent_space}
[TODO]





\bibliographystyle{abbrvnat}
\bibliography{Intro2MachineLearning}


\end{document}