
\chapter{Unsupervised Learning}

\label{sec:unsupervised}

Unlike Supervised Learning, in Unsupervised Learning there is no outcome variable. 
There is thus no notion of ``right'' and ``wrong''. 
We merely want to learn the joint distribution of the data, $x \in \featureS$, and represent it in some way we can understand. 
Well, maybe ``merely'' is not the right word, as learning the joint distribution of the data means that instead of learning the relation between a set $x$ and another, $y$, we now try to learn the relation between all pairs of variables in $x$, which is clearly more challenging. 

Describing the data via it's joint distribution is the pinnacle, but would require many samples if $x$ has a high dimension (by high we mean $p>3$!). For higher dimensions we need to set more modest goals, which vary according to the purpose of the analysis.

The different goals of unsupervised learning can be
\begin{enumerate}
\item \textbf{Density Estimation}: Estimate $\dist(x)$.
\item \textbf{High density regions}: Find feature combinations which tend to concentrate, hopefully, because the belong to homogenous and interpretable subgroups.
\item \textbf{Low dimensional representation}: Find a low dimensional representation of the joint distribution. This allows the learning from realistic sample sizes, and analysis by, e.g., interpretable parameters and visualization.
Since this also serves in supervised learning, it is discussed with more generality in Appendix~\ref{apx:dim_reduce}.
\item \textbf{Cluster}: Assign observation to homogenous groups. In particular, the particular features of an observation are no longer of interest, once it has been assigned to a cluster.
\end{enumerate}


\paragraph{Relation to Supervised Classification}
Learning the joint data distribution, i.e., density estimation, is not only a goal for itself, but can serve for classification. 
The full data generating distribution, in the context of \emph{supervised} learning is called the \emph{generative model}.\marginnote{Generative Model}
See Appendix \ref{apx:generative_concept} for the usage of the estimated generative model for classification.




\section{Cluster Analysis}
\label{sec:cluster_analysis}

In cluster analysis, we aim at assigning observations to (hopefully) homogenous and meaningful clusters. 

Groups identified are called \emph{clusters}, so that these methods are known as \emph{cluster analysis}, or \emph{data segmentation} methods.

Cluster analysis is typically easier than learning a joint distribution (\S\ref{sec:density_estimation}) or even detecting high density regions (\S\ref{sec:high_density}). 
We will see that for cluster analysis, we don't need the actual features $X$; as it will turn out, many methods only require some notion of distance between the points, and not their actual features. 
For this reason, clustering is intimately related to graphs and \emph{networks}\footnote{A graph describes a yes/no relation. A network is a weighted graph, which not only describes the existence of a relation, but also its strength.}, which capture only the similarity or dissimilarity of the observations, and not their actual features.\marginnote{Network}



\subsection{K-Means Clustering}
\label{sec:kmeans}
The idea behind K-means clustering is to find a representative point for each of K clusters, and assign each (unlabeled) data point to one of these clusters. As each cluster has a representative point, this is also a \emph{prototype method}.\marginnote{Prototype Methods}.
The clusters are defined so that they minimize the average distance between all points to the center of the cluster.

In K-means, the clusters are first defined, and then similarities computed. This is thus a \emph{top-down} method.\marginnote{Top Down Clustering}

\begin{algorithm}[H]
\caption{K-Means}
\begin{algorithmic}
\State Choose the number of clusters $K$.
\State Arbitrarily assign points to clusters.
\While {Clusters keep changing}
	\State Compute the cluster centers as the average of their points.
	\State Assign each point to its closest cluster center (in Euclidean distance).
\EndWhile
\State \Return Cluster assignments and means.
\end{algorithmic}
\end{algorithm}





\subsection{K-Medoids Clustering}
\label{sec:k_medoids}


If a Euclidean distance is inappropriate for a particular $\featureS$, or that robustness to corrupt observations is required, or that we wish to constrain the cluster centers to be actual observations, then the \emph{K-Medoids} algorithm is an adaptation of K-means that allows this.

\begin{algorithm}[H]
\caption{K-Medoids}
\begin{algorithmic}
\State Choose a similarity metric $\similarity(x_i,x_j)$.
\State Choose the number of clusters $K$.
\State Arbitrarily assign points to clusters.
\While {Clusters keep changing}
	\State Within each cluster, set the center as the data point that minimizes the sum of distances to other points in the cluster.
	\State Assign each point to its closest cluster center (in $\similarity(x_i,x_j)$ distance).
\EndWhile
\State \Return Cluster assignments and centers.
\end{algorithmic}
\end{algorithm}


See Section 14.3.10 in \cite{hastie_elements_2003}.








\subsection{Hierarchical Clustering}
\label{sec:hierarchical}
[TODO]








\subsection{Spectral Clustering}
\label{sec:spectral_clustering}
[TODO]





\subsection{Self Organizing Maps (SOM)}
\label{sec:som}
SOMs are aimed at learning low dimensional representations of data useful for clustering, using the original features $X$.
As such, they take the form of K-means clustering (\S\ref{sec:kmeans}), but not in the original space $\featureS$ but rather in a low dimensional representation $\manifold$.


\paragraph{Mathematics of SOMs}
[TODO]

For more on SOMs see Section 14.4 in \cite{hastie_elements_2003}. 









\section{Detect High Density Regions}
\label{sec:high_density}

When data data is high dimensional so that we cannot perform density estimation, or when we are merely interested in grouping observations, we can still detect high density regions where observations ``clump'' together. Hopefully, into meaningful homogenous groups.
The following methods are thus also known a \emph{bump hunting} or \emph{mode finding}.\marginnote{Bump Hunting}
They are all different heuristic algorithms that return high density regions. 

Once high density regions have been detected, they could be assigned to clusters. Then again, assigning to clusters is typically an easier problem which can be solved directly (see \S\ref{sec:cluster_analysis}).



\subsection{Association Rules}
\label{sec:association}
Association rules, or \emph{market basket analysis}, or \emph{affinity analysis}, can be seen as approximating the joint distribution with a region-wise constant function (Eq.~\ref{eq:decision_list}).\marginnote{Market Basket Analysis}
Put differently, we want to capture high density regions of the joint distribution of $x$ with by approximating it with a decision-list (not tree).
Learning a decision-list is a computationally impractical problem in general. Association rules are thus typically learned over binary feature spaces $\featureS$, using heuristic optimization schemes.

This type of problems typically occurs in sales analysis, where vendors seek for combinations of products that tend to sell jointly (so that can design the store better, or discount product bundles).

The \emph{Aprioiri} algorithm \cite{agraval_fast_1994}, is an example of such a heuristic search for high density combinations.\marginnote{Apriori Algorithm}






% % % Density Estimation % % % % 
\section{Density Estimation}
\label{sec:density_estimation}

We now aim at the possibly hardest target in unsupervised learning- estimating $\dist(x)$.
As previously stated, learning $\dist$ is of interest for itself, but can also serve for classification (\S\ref{sec:generative}), for detecting high density regions (\S\ref{sec:high_density}), and for clustering (\S\ref{sec:cluster_analysis}).




\subsection{Parametric Density Estimation}
If a parametric generative model can be assumed, this collapses to the parameter estimation problem in the classical statistical literature (\S\ref{sec:estimation}). Maximum Likelihood estimation being a particularly attractive approach, but certainly not the only one.
If a parametric model cannot be assumed, we fall into the realm of non-parametric methods. 
As we saw for supervised learning, these typically rely on pooling information from neighbourhoods of $\featureS$.



\subsection{Kernel Density Estimation}
\label{sec:kernel_density}

Much like the Kernel Regression regression, a \naive estimator is the moving average.
A natural generalization, in the spirit of the Nadaraya-Watson smoother (Eq.(\ref{eq:nadaraya_watson})) is the \emph{Parzen} estimate:\marginnote{Parzen Estimate}
\begin{align}
\label{eq:parzen}
	\estim{\pdf}(x):=\frac{1}{n \lambda} \sum_{i=1}^n \kernel_\lambda(x,x_i).
\end{align}

\begin{remark}
If you have been convinced by the use of KNN (\S\ref{sec:knn}) for regression (or classification), there is no reason not to use it for density estimation. It will keep offering the same pros and cons as in KNN regression (or classification).
\end{remark}

As previously stated, these methods may fail when $x$ are high dimensional. We thus recur to other methods.



\subsection{Graphical Models}
\label{sec:graphical_model}
[TODO]

















\section{Dimensionality Reduction to Linear Subspaces}
\label{sec:dim_reduce_linear}

The idea of ERM and Inductive Bias also applies to unsupervised learning.
If $\hyp(x)$ is low dimensional representation of $x$, mapping it to a ``simple'' manifold $\manifold$, we would seek some $\hyp$ that does not incur too much loss, on average. I.e., we seek to minimize $\risk(\hyp)$.

As usual, we do not have access to the data generating process of $x$, so we typically content ourselves with the empirical risk minimization.
In the context of unsupervised learing, the empirical risk is known as the \emph{reconstruction error}.\marginnote{Reconstruction Error}

The building blocks of a supervised learning problem are: 
(i) the loss function $\loss$,
(ii) the hypothesis class $\hypclass$, 
(iii) a regularization level, and
(iv) the optimization scheme.
In unsupervised learning, we have similar building blocks. We still have some loss function, regularized or not, penalizing for the poor reconstruction of the data. We can still solve an ERM problem, or replace the optimization by some other learning algorithm. The main difference is in that instead of some low dimensional mapping $\hyp:x \mapsto \estim{y}$, we learn a mapping  $\hyp:x \mapsto \estim{x}$.


For a general discussion of the idea of dimensionality reduction see Appendix~\ref{apx:dim_reduce}.





\subsection{Principal Components Analysis (PCA)}
\label{sec:pca}

PCA is such a basic technique, it has been rediscovered and renamed independently in many fields. 
It can be found under the names of \emph{discrete Karhunen–Loève Transform; Hotteling Transform; Proper Orthogonal Decomposition (POD); Eckart–Young Theorem; Schmidt–Mirsky Theorem;  Empirical Orthogonal Functions; Empirical Eigenfunction Decomposition;  Empirical Component Analysis;  Quasi-Harmonic Modes;  Spectral Decomposition;  Empirical Modal Analysis;} and possibly more\footnote{Wikipedia: \url{http://en.wikipedia.org/wiki/Principal_component_analysis} }.
The many names are quite interesting as they offer an insight into the different problems that led to its (re)discovery.

Starting with an example, consider human height and weight data. 
While clearly two dimensional data, you don't really need both to understand how ``big'' are the people in the data. 
This is because, height and weight vary mostly along a single dimension, which can be interpreted as the ``bigness'' of an individual. 
This is why, physicians use the Body Mass Index (BMI) as an indicator of size, instead of a two-dimensional measurement.
Assume you now wish to give each individual a size score, that is a linear combination of height and weight: PCA does just that. It returns the linear combination that has the most variability, i.e., the combination which best distinguishes between individuals. 


\paragraph{Terminology}
PCA has received much attention. As such, it has rich underlying theory and terminology.
Here are some terms needed to understand PCA outputs:
\begin{itemize}
\item[Principal Components] The linear combinations of the features, which best separate between observations. In our example- the ``bigness'' index of each individual. The first components captures the most variance, the second components, the second most-variance, etc. In terms of $\manifold$, the principal components are an orthogonal basis for $\manifold$.
\item[Scores] Synonymous to Principal Components.
\item[Loadings] The weights of each data point in each principal component. In our example, the importance of the height and weight in constructing the ``bigness'' index.
\end{itemize}


\paragraph{Intuition}
Notice we have currently offered two motivations for PCA: 
(i) Find linear combinations that best distinguish between observations, i.e., maximize variance. 
(ii) Find the linear subspace the bets approximates the data.
The reason these two problems are equivalent, is due to the use of the squares error.
Informally speaking, the data has some total variance. This variance can be decomposed into the part captured in $\manifold$, and the part not captured\footnote{Analogous to $SST=SSR+SSE$ in linear regression.}. 
Since the variance in the data consists of sums of squares, minimizing the distance from $X$ to $\manifold$, is the same as maximizing the variance of $X \project \manifold$, since their sum is fixed.


\subsubsection{Mathematics of PCA}

PCA seeks to represent $x$ in the simplest manifold possible: a linear subspace. Thus, $\manifold$ is simply a rank $\rank$ linear subspace in $\mathbb{R}^p$.
A low dimensional representation of the points in the data consists of decoding their low dimensional encoding. 
Since we are dealing with linear spaces, encoding and decoding can be represented with matrices. 
We thus denote the encoding operator by $\encode:\featureS \mapsto \reals^\rank$, a $\rank \times p$ matrix.
We also denote the decoding operator by $\decode:\reals^\rank \mapsto \featureS$ , a $p \times \rank$ matrix. 
Our predictor function ($\hyp$) in this case is $\hyp(x_i)=\decode\encode x_i$.
 
The ERM problem:
\begin{align}
\label{eq:PCA}
	\argmin{\decode,\encode}{\frac{1}{n}\sum_i (x_i-\decode\encode x_i)^2} .
\end{align}

Without proof, it turns out that Eq.(\ref{eq:PCA}) has a closed form solution. 
This solution is typically presented using the Singular Value Decomposition (SVD) of $X$ we now define.\marginnote{SVD}
\begin{definition}[SVD]
Any $n \times p$ matrix $X$, can be decomposed into $X=UDV'$ where 
$U$ is an $n \times p$ orthogonal matrix ($U'U=I_p$); 
$D$ is a $p \times p$ diagonal matrix with diagonal elements $d_1 \geq d_2 \geq \dots \geq d_p$;
$V$ is a $p \times p$ orthogonal matrix ($V'V=I_p$).
\end{definition}

The solution to Eq.(\ref{eq:PCA}) is 
\begin{align}
\label{eq:PCA_solution}
	\estim{\encode} =& V_\rank' \\
	\estim{\decode} =& V_\rank,
\end{align}
where $V_\rank$ are the first $\rank$ columns of $V$ in the Singular Value Decomposition of $X$.









\subsection{Sparse Principal Component Analysis (sPCA)}
[TODO]






\subsection{Multidimensional Scaling (MDS)}
\label{sec:MDS}

In MDS we take a distance network between observations, and try to embed it in a lower dimensional linear subspace, while preserving the original distances.
Unless the data really is of a low dimension, the embedding may distort the original distances, and may even change the ordering of the observations. The good news is we will be able to visualize \andor cluster them in their new simplified representation. 
If employed for the purpose of clustering, it is very similar in nature to SOMs (\S\ref{sec:som}), but differs in that it requires only the similarity network $\similaritys$, and not the actual features $X$. 

The embedding is merely the assigning of each point to a location in some lower dimensional space. The assignment is driven by a \emph{stress function} which penalizes for the average distortion in pair-wise distances.


\paragraph{Mathematics of MDS}
Starting with either a dissimilarity network $\dissimilaritys=(\dissimilarity_{i,j})$, or a similarity network $\similaritys=(\similarity_{i,j})$.
Similarities can be though of as correlations, and dissimilarities as Euclidean distances (which are indeed the typical measures in use).
Define $z_i \in \reals^\rank$ the location of point $i$ in the target linear space of rank $\rank$. 
The $z_i$'s are set to minimize the \emph{stress} function.
Typical stress functions include:
\begin{description}
\item[Classical Scaling] Using the centred inner product (i.e. empirical covariance) as the similarity measure and squared error for stress:
$\similarity_{i,j}:= \scalar{x_i-\bar x}{x_j-\bar{x}}$ and the new location are given by
\begin{align}
\label{eq:stress_classical}
	 \argmin{z_1,\dots,z_n}{\sum_{i,j=1}^{n} (\similarity_{i,j}-\scalar{z_i-\bar z}{z_j-\bar{z}} )^2}.
\end{align}
\item[Kruskal-Shepard Scaling] Using squared error for stress:
\item[Sammon Mapping] Using the proportion of squared error for stress:
\end{description}


\begin{remark}[MDS and PCA]
content...
\end{remark}


For more on MDS see Section 14.8 in \cite{hastie_elements_2003}.


% % % % Non linear dimensionality reduction % % % % %
\section{Dimensionality Reduction to Non Linear Subspaces}
\label{sec:dim_reduce_nonlinear}


\subsection{Kernel Principal Component Analysis (kPCA)}
\label{sec:kpca}

Back to the motivating example from the PCA section (\S\ref{sec:pca}): assume we want to construct a ``bigness'' score, that best separates between individuals, but we no longer constrain it to be a linear function of the height and weight.
We could thus try to solve 
\begin{align}
\label{eq:kpca_wrong}
	\argmax{g}{\covn{g(X)}}
\end{align}
where $\covn{g(X)}$ is the empirical covariance of $g$ applied on each individual, i.e., row-wise on $X$.

Alas, just like in the supervised learning problem, without any constraints on $g$, we might overfit \andor not be able to compute $g$ as optimization is done in a infinite dimensional space. 
We also need to recall that if we allow for a $\rank$ dimensional score, then we have to solve Eq.(\ref{eq:kpca_wrong}) $\rank$ times.
We thus have two matters to attend:
(i) We need to constrain $g$ so that it does not overfit.
(ii) We need the problem to be computable.
This is precisely what kPCA does. 
It turns out, that for a particular class of functions $g$, the optimal $g$'s take a simple form. 
The classes of such $g$'s are known as Reproducing Kernel Hilbert Spaces. They are presented in Appendix~\ref{apx:rkhs}.

The fundamental observation allowing the application of RKHS theory, is that the scores in the PCA problem, actually depend only on the similarities between individuals, as measured by their empirical correlation. 
In kPCA we replace the measure of similarity with some other measure called the \emph{kernel}, chosen from a particular class, and then proceed as if we were solving a PCA problem.


\paragraph{Mathematics of kPCA}
[TODO]



\subsection{Principal Curves}
[TODO]


\subsection{ISOmap}
[TODO]


\subsection{LLE}
[TODO]



\subsection{Random Projections}
[TODO: Johnson-Lindenstrauss Lemma]



\subsection{Compressed Sensing}
\label{sec:compressed_sensing}
[TODO]



\subsection{Information Bottleneck}
[TODO]


\begin{remark}[Information Bottleneck and ICA]
[TODO]
\end{remark}





%%%%%%%%% Generative Models %%%%%%%%%%%



\section{Latent Space Generative Models}
\label{sec:latent_space}

We have already met generative models for supervised learning (\S\ref{sec:generative}), and for unsupervised learning (\S\ref{sec:density_estimation}).
We now discuss a class of generative models, that can be seen as a dimensionality device. 
The following models and methods assume that the data generating process is governed by some low dimensional unobservable \emph{state}, also known as \emph{latent variable}, or \emph{hidden variable}.
The latent variables define a \emph{state}. This is why these models are also known as \emph{state space} models. A term coined by \citet{kalman_contributions_1960}.\marginnote{State-Space Models}

This specification allows us to:
(i) \emph{Interpret} the data generating process via its states. 
(ii) \emph{Formulate} the density estimation problem as a low dimensional problem, we can actually hope to solve, even when the data itself is very high dimensional.

The fundamental idea is that while the data generating distribution may have a complicated form, when we condition on the unobserved variable, the distribution greatly simplifies. 


\subsection{Mixtures}
[TODO]


\subsection{Regression Switching}
[TODO]



\subsection{Factor Analysis (FA)}
\label{sec:factor_analysis}

Factor Analysis is solved very similarly to PCA (\S\ref{sec:pca}), so that the two are often confused and interchanged. 
FA, however, stems from a rather different motivation than PCA.
In can be seen as a descriptive method using some linear algebra properties.
FA is a generative method, estimable using maximum likelihood. In some cases, the two methods can collapse to the same problem. 
We already know this can happen as we have seen that the Linear Regression problem with Gaussian disturbances can collapse to an OLS problem. 

In FA we assume that the observed $X$'s depend linearly on a set of $\rank$ independent latent (i.e. unobservable) attributes we denote with $\latent$.
The generative model is thus
\begin{align}
\label{eq:factor}
	X=\loadings \latent+\varepsilon
\end{align}
Assuming a generative distribution on $\latent$ and $\varepsilon$, we may try to estimate $\loadings \latent$ by maximum likelihood.
Sadly, recovering the particular latent attributes $\latent$ from $\estim{\loadings\latent}$ is still impossible as there are infinitely many such solutions. To see this, consider an orthogonal \emph{rotation} matrix $\rotation$ ($\rotation' \rotation=I$). For each such $\rotation$: $ \loadings \latent=\loadings \rotation' \rotation \latent = \loadings^* \latent^*$.

The choice of $\rotation$ changes the interpretation of the latent attributes. This is why many researchers find FA an unsatisfactory inference tool.



\paragraph{PCA Terminology}
\begin{itemize}
\item Factors: The unobserved attributes $\latent$.
\item Loadings: The $A$ matrix; the contribution of each attribute to the observed $X$.
\item Rotation: An orthogonal re-combination of the latent attributes $\latent$ and loadings, which changes the interpretation of the result.
\end{itemize}




\subsection{Independent Component Analysis (ICA)}
ICA aims at overcoming the non-uniquness of the latent attributes in FA (\S\ref{sec:factor_analysis}).
The fundamental observation is that the non-uniqueness (i.e., non identifiability of $\latent$) stems from the fact that the independent Gaussian distribution is invariant under rotation. I.e. for $\rv \latent \sim \gauss{0,\sigma^2 I}$ is the same as that of $\rotation \rv \latent$. 
By replacing the Gaussianity assumption on the distribution of $\rv \latent$, by some other multivariate distribution, coordinate-wise independent, we can identify $\latent$. 
This is precisely what ICA does. 

\begin{remark}[Blind Source Separation]
ICA is a popular technique in signal processing, where $\latent$ is actually the signal (e.g. sound) produced by several different sources. Recovering $\latent$ is thus recovering the original signals mixing in the recorded $X$. This is known as \emph{blind source separation}.
\end{remark}

\begin{remark}[ICA and FA]
The solutions to the ICA problem can ultimately be seen as a solution to the FA problem with a particular rotation $\rotation$.
Put differently, by the formulation of the ICA problem, implies a specific rotation so that $\latent$ is identifiable. 
\end{remark}



\subsubsection{Mathematics of ICA}
[TODO]


For more on ICA, see \cite{hyvarinen_independent_2000}. 







\subsection{Hidden Markov Models (HMM)}
\label{sec:hmm}
[TODO]





\subsection{Random Graphs}
\label{sec:random_graphs}
[TODO]


\subsubsection{\erdos \renyi}
[TODO]



\subsubsection{Exponential Random Graphs (ERGMs)}
[TODO]


\begin{remark}[Relation to Spectral Clustering]
[TODO]
\end{remark}


\subsection{Collaborative Filtering}
\label{sec:collaborative_filtering}
[TODO]
