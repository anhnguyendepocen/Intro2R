---
title: "Unsupervised Learning"
author: "Jonathan Rosenblatt"
date: "April 12, 2015"
output: html_document
---

Some utility functions:
```{r utility}
l2 <- function(x) x^2 %>% sum %>% sqrt 
l1 <- function(x) abs(x) %>% sum  
MSE <- function(x) x^2 %>% mean 

# Matrix norms:
frobenius <- function(A) norm(A, type="F")
spectral <- function(A) norm(A, type="2")
```


__Note__: `foo::bar` means that function `foo` is part of the `bar` package. 
With this syntax, there is no need to load (`library`) the package.
If a line does not run, you may need to install the package: `install.packages('bar')`.
Sadly, RStudio currently does not autocomplete function arguments when using the `::` syntax.




# Learning Distributions 

## Gaussian Density Estimation
```{r}
# Sample from a multivariate Gaussian:
## Generate a covariance matrix
p <- 10
Sigma <- bayesm::rwishart(nu = 100, V = diag(p))$W
lattice::levelplot(Sigma)
# Sample from a multivariate Gaussian:
n <- 1e3
means <- 1:p
X1 <- mvtnorm::rmvnorm(n = n, sigma = Sigma, mean = means)
dim(X1)

# Estiamte parameters and compare to truth:
estim.means <- colMeans(X1) # recall truth is (10,...,10)
plot(estim.means~means); abline(0,1, lty=2)

estim.cov <- cov(X1)
estim.cov.errors <- Sigma - estim.cov
lattice::levelplot(estim.cov.errors)

plot(estim.cov~Sigma); abline(0,1, lty=2)

frobenius(estim.cov.errors)

# Now try the same while playing with n and p.
```



Other covariance estimators (robust, fast,...)
```{r covariances}
# Robust covariance
estim.cov.1 <- MASS::cov.rob(X1)$cov
estim.cov.errors.1 <- Sigma - estim.cov.1
lattice::levelplot(estim.cov.errors.1)
frobenius(estim.cov.errors.1)

# Nearest neighbour cleaning of outliers
estim.cov.2 <- covRobust::cov.nnve(X1)$cov
estim.cov.errors.2 <- Sigma - estim.cov.2
lattice::levelplot(estim.cov.errors.2)
frobenius(estim.cov.errors.2)


# Regularized covariance estimation
estim.cov.3 <- robustbase::covMcd(X1)$cov
estim.cov.errors.3 <- Sigma - estim.cov.3
lattice::levelplot(estim.cov.errors.3)
frobenius(estim.cov.errors.3)


# Another robust covariance estimator
estim.cov.4 <- robustbase::covComed(X1)$cov
estim.cov.errors.4 <- Sigma - estim.cov.4
lattice::levelplot(estim.cov.errors.4)
frobenius(estim.cov.errors.4)
```

## Non parametric density estimation
There is nothing that will even try dimensions higher than 6.
See [here](http://vita.had.co.nz/papers/density-estimation.pdf) for a review.


## Association rules
Note: Visualization examples are taken from the arulesViz [vignette](http://cran.r-project.org/web/packages/arulesViz/vignettes/arulesViz.pdf)

```{r association rules}
library(arules)
data("Groceries")
inspect(Groceries[1:2])
summary(Groceries)

rules <- apriori(Groceries, parameter = list(support=0.001, confidence=0.5))
summary(rules)
rules %>% sort(by='lift') %>% head %>% inspect


# Select a subset of rules
rule.subset <- subset(rules, subset = rhs %pin% "yogurt")
inspect(rule.subset)

# Visualize rules:
library(arulesViz)
plot(rules)

subrules <- rules[quality(rules)$confidence > 0.8]
plot(subrules, method="matrix", measure="lift", control=list(reorder=TRUE))
plot(subrules, method="matrix", measure=c("lift", "confidence"), control=list(reorder=TRUE))

plot(subrules, method="grouped")
plot(rules, method="grouped", control=list(k=50))

subrules2 <- head(sort(rules, by="lift"), 10)
plot(subrules2, method="graph", control=list(type="items"))
plot(subrules2, method="graph")

# Export rules graph to use with other software:
# saveAsGraph(head(sort(rules, by="lift"),1000), file="rules.graphml")

rule.1 <- rules[1] 
inspect(rule.1)
plot(rule.1, method="doubledecker", data = Groceries)
```

See also the `prim.box` function in the `prim` package for more algorithms to learn association rules



# Dimensionality Reduction

## PCA
Note: example is a blend from [Gaston Sanchez](http://gastonsanchez.com/blog/how-to/2012/06/17/PCA-in-R.html) and [Georgia's Geography dept.](http://geog.uoregon.edu/GeogR/topics/pca.html).


Get some data
```{r PCA data}
?USArrests

plot(USArrests) # basic plot
corrplot::corrplot(cor(USArrests), method = "ellipse") # slightly fancier


# As a correaltion graph
cor.1 <- cor(USArrests)
qgraph::qgraph(cor.1) 
qgraph::qgraph(cor.1, layout = "spring", posCol = "darkgreen", negCol = "darkmagenta")

```


```{r PCA}
USArrests.1 <- USArrests[,-3] %>% scale
pca1  <-  prcomp(USArrests.1, scale. = TRUE)

(pca1$rotation) # loadings

# Now score the states:
pca1$x %>% extract(,1) %>% sort %>% head 

```
Interpretation:

- PC1 seems to capture overall crime rate.
- PC2 seems distinguish between sexual and non-sexual crimes



Projecting on first two PCs:
```{r visualizing PCA}
library(ggplot2) # for graphing

pcs  <-  as.data.frame(pca1$x)
ggplot(data = pcs, aes(x = PC1, y = PC2, label = rownames(pcs))) +
  geom_hline(yintercept = 0, colour = "gray65") +
  geom_vline(xintercept = 0, colour = "gray65") +
  geom_text(colour = "tomato", alpha = 0.8, size = 4) +
  ggtitle("PCA plot of USA States - Crime Rates")
```


The bi-Plot
```{r biplot}
biplot(pca1) #ugly!


# library(devtools)
# install_github("vqv/ggbiplot")
ggbiplot::ggbiplot(pca1, labels =  rownames(USArrests.1)) # better!
```


The scree-plot
```{r screeplot}
ggbiplot::ggscreeplot(pca1)
```
So clearly the main differentiation 


Visualize the scoring as a projection of the states' attributes onto the factors.
```{r}
# get parameters of component lines (after Everitt & Rabe-Hesketh)
load <- pca1$rotation
slope <- load[2, ]/load[1, ]
mn <- apply(USArrests.1, 2, mean)
intcpt <- mn[2] - (slope * mn[1])

# scatter plot with the two new axes added
dpar(pty = "s")  # square plotting frame
USArrests.2 <- USArrests[,1:2] %>%  scale
xlim <- range(USArrests.2)  # overall min, max
plot(USArrests.2, xlim = xlim, ylim = xlim, pch = 16, col = "purple")  # both axes same length
abline(intcpt[1], slope[1], lwd = 2)  # first component solid line
abline(intcpt[2], slope[2], lwd = 2, lty = 2)  # second component dashed
legend("right", legend = c("PC 1", "PC 2"), lty = c(1, 2), lwd = 2, cex = 1)

# projections of points onto PCA 1
y1 <- intcpt[1] + slope[1] * USArrests.2[, 1]
x1 <- (USArrests.1[, 2] - intcpt[1])/slope[1]
y2 <- (y1 + USArrests.1[, 2])/2
x2 <- (x1 + USArrests.1[, 1])/2
segments(USArrests.1[, 1], USArrests.1[, 2], x2, y2, lwd = 2, col = "purple")
```


Visualize the loadings:
```{r}
# install.packages('GPArotation')
pca.qgraph <- qgraph::qgraph.pca(USArrests.1, factors = 2, rotation = "varimax")
plot(pca.qgraph)

qgraph::qgraph(pca.qgraph, posCol = "darkgreen", layout = "spring", negCol = "darkmagenta", 
    edge.width = 2, arrows = FALSE)
    ```




More implementations of PCA:
```{r}
# FAST solutions:
gmodels::fast.prcomp()

# More detail in output:
FactoMineR::PCA()

# For flexibility in algorithms and visualization:
ade4::dudi.pca()

# Another one...
install.packages('amap')
amap::acp()

```



Principal tensor analysis:
```{r PTA}
PTAk::PTAk()
```



## sPCA
```{r sPCA}

```


## kPCA
```{r kPCA}
kernlab::kpca()
```


## Random Projections 
```{r Random Projections}

```


## MDS
```{r MDS}
stats::cmdscale()

MASS::sammon()
MASS::isoMDS()
```


## Isomap
```{r Isomap}

```


## LLE
```{r LLE}

```

## LocalMDS
```{r Local MDS}

```

## Principal Curves & Surfaces
```{r Principla curves}

```





# Latent Space Generative Models

## FA
```{r factor analysis}
psych::principal()
```

## ICA
```{r ICA}
fastICA::fastICA() # Also performs projection pursuit

```



## Exploratory Projection Pursuit
```{r exploratory projection pursuit}
install.packages('REPPlab')
library(REPPlab) % will require the `rJava` package

```

## Generative Topographic Map
[TODO]

## Finite Mixture
```{r mixtures}
install.packages('mixtools')
library(mixtools)

```
Read [this](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf) for more information.


## HMM
```{r}
# install.packages('HiddenMarkov')
library(HiddenMarkov)


```



# Clustering:

Generate clusters:
```{r generate clusters}
X <- clusterGeneration::genRandomClust(numClust=2)

clusterGeneration::viewClusters(X, cl=2)
```



## K-means
```{r kmeans}
stats::kmeans()

```

## Kmeans++
```{r kmeansPP}
kmpp <- function(X, k) {
  n <- nrow(X)
  C <- numeric(k)
  C[1] <- sample(1:n, 1)
  
  for (i in 2:k) {
    dm <- distmat(X, X[C, ])
    pr <- apply(dm, 1, min); pr[C] <- 0
    C[i] <- sample(1:n, 1, prob = pr)
  }
  
  kmeans(X, X[C, ])
}
```


## K-medoids
```{r kmedoids}
cluster::pam()

# Many other similarity measures:
proxy::dist()
```

## Hirarchial
```{r}
hclust()

# install.packages('cluster')
library(cluster)
agnes()

```


## Self Organizing Maps
You may note the similar function names. This is why the `::` syntax is very useful.
```{r SOM}
# install.packages('som')
library(som)
som::som()

kohonen::som()

class::SOM()
```



## Spectral Clustering
```{r}
# install.packages('kernlab')
library(kernlab)

specc()
```

